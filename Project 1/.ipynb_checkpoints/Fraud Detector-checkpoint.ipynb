{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (590540, 213) test shape (506691, 213)\n",
      "CPU times: user 38.8 s, sys: 1.93 s, total: 40.7 s\n",
      "Wall time: 40.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD TRAIN\n",
    "X_train = pd.read_pickle('../input/ieee-fraud-detection/train_transaction.pkl',index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\n",
    "train_id = pd.read_pickle('../input/ieee-fraud-detection/train_identity.pkl',index_col='TransactionID', dtype=dtypes)\n",
    "X_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n",
    "# LOAD TEST\n",
    "X_test = pd.read_pickle('../input/ieee-fraud-detection/test_transaction.pkl',index_col='TransactionID', dtype=dtypes, usecols=cols)\n",
    "test_id = pd.read_pickle('../input/ieee-fraud-detection/test_identity.pkl',index_col='TransactionID', dtype=dtypes)\n",
    "fix = {o:n for o, n in zip(test_id.columns, train_id.columns)}\n",
    "test_id.rename(columns=fix, inplace=True)\n",
    "X_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n",
    "# TARGET\n",
    "y_train = X_train['isFraud'].copy()\n",
    "del train_id, test_id, X_train['isFraud']; x = gc.collect()\n",
    "# PRINT STATUS\n",
    "print('Train shape',X_train.shape,'test shape',X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "X_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "X_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n",
    "\n",
    "X_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "X_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Magic Feature - UID\n",
    "We will now create and use the MAGIC FEATURES. First we create a UID which will help our model find clients (credit cards). This UID isn't perfect. Many UID values contain 2 or more clients inside. However our model will detect this and by adding more splits with its trees, it will split these UIDs and find the single clients (credit cards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['day'] = X_train.TransactionDT / (24*60*60)\n",
    "X_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n",
    "\n",
    "X_test['day'] = X_test.TransactionDT / (24*60*60)\n",
    "X_test['uid'] = X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "cols = list( X_train.columns )\n",
    "cols.remove('TransactionDT')\n",
    "for c in ['D6','D7','D8','D9','D12','D13','D14']:\n",
    "    cols.remove(c)\n",
    "for c in ['oof','DT_M','day','uid']:\n",
    "    cols.remove(c)\n",
    "    \n",
    "# FAILED TIME CONSISTENCY TEST\n",
    "for c in ['C3','M5','id_08','id_33']:\n",
    "    cols.remove(c)\n",
    "for c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n",
    "    cols.remove(c)\n",
    "for c in ['id_'+str(x) for x in range(22,28)]:\n",
    "    cols.remove(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-3-78b6d0d21102>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-78b6d0d21102>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    def encode_AG(main_columns, uids, aggregations=['mean'], train_df, test_df,\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FREQUENCY ENCODE TOGETHER\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "        print(nm,', ',end='')\n",
    "        \n",
    "# LABEL ENCODE\n",
    "def encode_LE(col,train,test,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "    if verbose: print(nm,', ',end='')\n",
    "        \n",
    "        \n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n",
    "def encode_AG(main_columns, uids, aggregations=['mean'], train_df, test_df, \n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "                \n",
    "# COMBINE FEATURES\n",
    "def encode_CB(col1,col2,df1,df2):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "    encode_LE(nm,verbose=False)\n",
    "    print(nm,', ',end='')\n",
    "    \n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(col+'_'+main_column+'_ct, ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW USING THE FOLLOWING 263 FEATURES.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card5',\n",
       "       'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain',\n",
       "       'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9',\n",
       "       'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5',\n",
       "       'D10', 'D11', 'D15', 'M1', 'M2', 'M3', 'M4', 'M6', 'M7', 'M8',\n",
       "       'M9', 'V1', 'V3', 'V4', 'V6', 'V8', 'V11', 'V13', 'V14', 'V17',\n",
       "       'V20', 'V23', 'V26', 'V27', 'V30', 'V36', 'V37', 'V40', 'V41',\n",
       "       'V44', 'V47', 'V48', 'V54', 'V56', 'V59', 'V62', 'V65', 'V67',\n",
       "       'V68', 'V70', 'V76', 'V78', 'V80', 'V82', 'V86', 'V88', 'V89',\n",
       "       'V91', 'V107', 'V108', 'V111', 'V115', 'V117', 'V120', 'V121',\n",
       "       'V123', 'V124', 'V127', 'V129', 'V130', 'V136', 'V138', 'V139',\n",
       "       'V142', 'V147', 'V156', 'V160', 'V162', 'V165', 'V166', 'V169',\n",
       "       'V171', 'V173', 'V175', 'V176', 'V178', 'V180', 'V182', 'V185',\n",
       "       'V187', 'V188', 'V198', 'V203', 'V205', 'V207', 'V209', 'V210',\n",
       "       'V215', 'V218', 'V220', 'V221', 'V223', 'V224', 'V226', 'V228',\n",
       "       'V229', 'V234', 'V235', 'V238', 'V240', 'V250', 'V252', 'V253',\n",
       "       'V257', 'V258', 'V260', 'V261', 'V264', 'V266', 'V267', 'V271',\n",
       "       'V274', 'V277', 'V281', 'V283', 'V284', 'V285', 'V286', 'V289',\n",
       "       'V291', 'V294', 'V296', 'V297', 'V301', 'V303', 'V305', 'V307',\n",
       "       'V309', 'V310', 'V314', 'V320', 'id_01', 'id_02', 'id_03', 'id_04',\n",
       "       'id_05', 'id_06', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13',\n",
       "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_28',\n",
       "       'id_29', 'id_31', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType',\n",
       "       'DeviceInfo', 'cents', 'addr1_FE', 'card1_FE', 'card2_FE',\n",
       "       'card3_FE', 'P_emaildomain_FE', 'card1_addr1',\n",
       "       'card1_addr1_P_emaildomain', 'card1_addr1_FE',\n",
       "       'card1_addr1_P_emaildomain_FE', 'TransactionAmt_card1_mean',\n",
       "       'TransactionAmt_card1_std', 'TransactionAmt_card1_addr1_mean',\n",
       "       'TransactionAmt_card1_addr1_std',\n",
       "       'TransactionAmt_card1_addr1_P_emaildomain_mean',\n",
       "       'TransactionAmt_card1_addr1_P_emaildomain_std', 'D9_card1_mean',\n",
       "       'D9_card1_std', 'D9_card1_addr1_mean', 'D9_card1_addr1_std',\n",
       "       'D9_card1_addr1_P_emaildomain_mean',\n",
       "       'D9_card1_addr1_P_emaildomain_std', 'D11_card1_mean',\n",
       "       'D11_card1_std', 'D11_card1_addr1_mean', 'D11_card1_addr1_std',\n",
       "       'D11_card1_addr1_P_emaildomain_mean',\n",
       "       'D11_card1_addr1_P_emaildomain_std', 'uid_FE',\n",
       "       'TransactionAmt_uid_mean', 'TransactionAmt_uid_std', 'D4_uid_mean',\n",
       "       'D4_uid_std', 'D9_uid_mean', 'D9_uid_std', 'D10_uid_mean',\n",
       "       'D10_uid_std', 'D15_uid_mean', 'D15_uid_std', 'C1_uid_mean',\n",
       "       'C2_uid_mean', 'C4_uid_mean', 'C5_uid_mean', 'C6_uid_mean',\n",
       "       'C7_uid_mean', 'C8_uid_mean', 'C9_uid_mean', 'C10_uid_mean',\n",
       "       'C11_uid_mean', 'C12_uid_mean', 'C13_uid_mean', 'C14_uid_mean',\n",
       "       'M1_uid_mean', 'M2_uid_mean', 'M3_uid_mean', 'M4_uid_mean',\n",
       "       'M5_uid_mean', 'M6_uid_mean', 'M7_uid_mean', 'M8_uid_mean',\n",
       "       'M9_uid_mean', 'uid_P_emaildomain_ct', 'uid_dist1_ct',\n",
       "       'uid_DT_M_ct', 'uid_id_02_ct', 'uid_cents_ct', 'C14_uid_std',\n",
       "       'uid_C13_ct', 'uid_V314_ct', 'uid_V127_ct', 'uid_V136_ct',\n",
       "       'uid_V309_ct', 'uid_V307_ct', 'uid_V320_ct', 'outsider15'],\n",
       "      dtype='<U45')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# FREQUENCY ENCODE UID\n",
    "encode_FE(X_train,X_test,['uid'])\n",
    "# AGGREGATE \n",
    "encode_AG(['TransactionAmt','D4','D9','D10','D15'],['uid'],['mean','std'],fillna=True,usena=True)\n",
    "# AGGREGATE\n",
    "encode_AG(['C'+str(x) for x in range(1,15) if x!=3],['uid'],['mean'],X_train,X_test,fillna=True,usena=True)\n",
    "# AGGREGATE\n",
    "encode_AG(['M'+str(x) for x in range(1,10)],['uid'],['mean'],fillna=True,usena=True)\n",
    "# AGGREGATE\n",
    "encode_AG2(['P_emaildomain','dist1','DT_M','id_02','cents'], ['uid'], train_df=X_train, test_df=X_test)\n",
    "# AGGREGATE\n",
    "encode_AG(['C14'],['uid'],['std'],X_train,X_test,fillna=True,usena=True)\n",
    "# AGGREGATE \n",
    "encode_AG2(['C13','V314'], ['uid'], train_df=X_train, test_df=X_test)\n",
    "# AGGREATE \n",
    "encode_AG2(['V127','V136','V309','V307','V320'], ['uid'], train_df=X_train, test_df=X_test)\n",
    "# NEW FEATURE\n",
    "X_train['outsider15'] = (np.abs(X_train.D1-X_train.D15)>3).astype('int8')\n",
    "X_test['outsider15'] = (np.abs(X_test.D1-X_test.D15)>3).astype('int8')\n",
    "print('outsider15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 withholding month 12\n",
      " rows of train = 453219 rows of holdout = 137321\n",
      "[0]\tvalidation_0-auc:0.757194\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[100]\tvalidation_0-auc:0.871341\n",
      "[200]\tvalidation_0-auc:0.892256\n",
      "[300]\tvalidation_0-auc:0.903352\n",
      "[400]\tvalidation_0-auc:0.907912\n",
      "[500]\tvalidation_0-auc:0.908055\n",
      "[600]\tvalidation_0-auc:0.907959\n",
      "Stopping. Best iteration:\n",
      "[417]\tvalidation_0-auc:0.908466\n",
      "\n",
      "Fold 1 withholding month 15\n",
      " rows of train = 488908 rows of holdout = 101632\n",
      "[0]\tvalidation_0-auc:0.832247\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[100]\tvalidation_0-auc:0.906459\n",
      "[200]\tvalidation_0-auc:0.929275\n",
      "[300]\tvalidation_0-auc:0.940543\n",
      "[400]\tvalidation_0-auc:0.944592\n",
      "[500]\tvalidation_0-auc:0.946015\n",
      "[600]\tvalidation_0-auc:0.94661\n",
      "[700]\tvalidation_0-auc:0.946441\n",
      "[800]\tvalidation_0-auc:0.946432\n",
      "Stopping. Best iteration:\n",
      "[601]\tvalidation_0-auc:0.946614\n",
      "\n",
      "Fold 2 withholding month 13\n",
      " rows of train = 497955 rows of holdout = 92585\n",
      "[0]\tvalidation_0-auc:0.809112\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[100]\tvalidation_0-auc:0.904115\n",
      "[200]\tvalidation_0-auc:0.923923\n",
      "[300]\tvalidation_0-auc:0.936195\n",
      "[400]\tvalidation_0-auc:0.940972\n",
      "[500]\tvalidation_0-auc:0.942581\n",
      "[600]\tvalidation_0-auc:0.942658\n",
      "[700]\tvalidation_0-auc:0.942671\n",
      "[800]\tvalidation_0-auc:0.94245\n",
      "Stopping. Best iteration:\n",
      "[647]\tvalidation_0-auc:0.943013\n",
      "\n",
      "Fold 3 withholding month 17\n",
      " rows of train = 501214 rows of holdout = 89326\n",
      "[0]\tvalidation_0-auc:0.805903\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[100]\tvalidation_0-auc:0.900972\n",
      "[200]\tvalidation_0-auc:0.925726\n",
      "[300]\tvalidation_0-auc:0.936672\n",
      "[400]\tvalidation_0-auc:0.941826\n",
      "[500]\tvalidation_0-auc:0.943173\n",
      "[600]\tvalidation_0-auc:0.943246\n",
      "[700]\tvalidation_0-auc:0.942995\n",
      "Stopping. Best iteration:\n",
      "[535]\tvalidation_0-auc:0.943457\n",
      "\n",
      "Fold 4 withholding month 14\n",
      " rows of train = 504519 rows of holdout = 86021\n",
      "[0]\tvalidation_0-auc:0.845517\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[100]\tvalidation_0-auc:0.920878\n",
      "[200]\tvalidation_0-auc:0.940643\n",
      "[300]\tvalidation_0-auc:0.949253\n",
      "[400]\tvalidation_0-auc:0.951569\n",
      "[500]\tvalidation_0-auc:0.951817\n",
      "[600]\tvalidation_0-auc:0.951515\n",
      "Stopping. Best iteration:\n",
      "[441]\tvalidation_0-auc:0.951908\n",
      "\n",
      "Fold 5 withholding month 16\n",
      " rows of train = 506885 rows of holdout = 83655\n",
      "[0]\tvalidation_0-auc:0.804213\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[100]\tvalidation_0-auc:0.909077\n",
      "[200]\tvalidation_0-auc:0.936265\n",
      "[300]\tvalidation_0-auc:0.949284\n",
      "[400]\tvalidation_0-auc:0.954733\n",
      "[500]\tvalidation_0-auc:0.956509\n",
      "[600]\tvalidation_0-auc:0.957115\n",
      "[700]\tvalidation_0-auc:0.957822\n",
      "[800]\tvalidation_0-auc:0.957817\n",
      "[900]\tvalidation_0-auc:0.957933\n",
      "[1000]\tvalidation_0-auc:0.957973\n",
      "[1100]\tvalidation_0-auc:0.957972\n",
      "Stopping. Best iteration:\n",
      "[962]\tvalidation_0-auc:0.958189\n",
      "\n",
      "####################\n",
      "XGB95 OOF CV= 0.9398187466957928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "oof = np.zeros(len(X_train))\n",
    "preds = np.zeros(len(X_test))\n",
    "\n",
    "skf = GroupKFold(n_splits=6)\n",
    "for i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n",
    "    month = X_train.iloc[idxV]['DT_M'].iloc[0]\n",
    "    print('Fold',i,'withholding month',month)\n",
    "    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=5000,\n",
    "        max_depth=12,\n",
    "        learning_rate=0.02,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.4,\n",
    "        missing=-1,\n",
    "        eval_metric='auc',\n",
    "        # USE CPU\n",
    "        #nthread=4,\n",
    "        #tree_method='hist'\n",
    "        # USE GPU\n",
    "        tree_method='gpu_hist' \n",
    "    )        \n",
    "    h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n",
    "                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n",
    "                verbose=100, early_stopping_rounds=200)\n",
    "    \n",
    "    oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n",
    "    preds += clf.predict_proba(X_test[cols])[:,1]/skf.n_splits\n",
    "    del h, clf\n",
    "    x=gc.collect()\n",
    "    print('#'*20)\n",
    "    print ('XGB95 OOF CV=',roc_auc_score(y_train,oof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n",
    "sample_submission.isFraud = preds\n",
    "sample_submission.to_csv('submissions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
