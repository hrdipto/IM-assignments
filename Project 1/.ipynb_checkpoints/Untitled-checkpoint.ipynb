{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from imblearn import over_sampling, under_sampling\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "train_transaction_data_file = \"./dataset/train_transaction.csv\"\n",
    "test_transaction_data_file = \"./dataset/test_transaction.csv\"\n",
    "train_identity_data_file = \"./dataset/train_identity.csv\"\n",
    "test_identity_data_file = \"./dataset/test_identity.csv\"\n",
    "sample_submission_file = \"./dataset/sample_submission.csv\"\n",
    "\n",
    "train_transaction_data = pd.read_csv(train_transaction_data_file)\n",
    "train_identity_data = pd.read_csv(train_identity_data_file)\n",
    "test_transaction_data = pd.read_csv(test_transaction_data_file)\n",
    "test_identity_data = pd.read_csv(test_identity_data_file)\n",
    "sample_submission = pd.read_csv(sample_submission_file)\n",
    "del train_transaction_data_file, test_transaction_data_file, train_identity_data_file, test_identity_data_file, sample_submission_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# reduce memory data function\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "            end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "train_transaction_data = reduce_mem_usage(train_transaction_data)\n",
    "train_identity_data = reduce_mem_usage(train_identity_data)\n",
    "test_transaction_data = reduce_mem_usage(test_transaction_data)\n",
    "test_identity_data = reduce_mem_usage(test_identity_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "#https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#lax`xansaction_data-579654\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train_transaction_data[c + '_bin'] = train_transaction_data[c].map(emails)\n",
    "    test_transaction_data[c + '_bin'] = test_transaction_data[c].map(emails)\n",
    "    \n",
    "    train_transaction_data[c + '_suffix'] = train_transaction_data[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test_transaction_data[c + '_suffix'] = test_transaction_data[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train_transaction_data[c + '_suffix'] = train_transaction_data[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test_transaction_data[c + '_suffix'] = test_transaction_data[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numerical = [col for col in numerical if col in train_transaction_data.columns]\n",
    "categorical = [col for col in categorical if col in train_transaction_data.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan2mean(df):\n",
    "    for x in list(df.columns.values):\n",
    "        if x in numerical:\n",
    "            #print(\"___________________\"+x)\n",
    "            #print(df[x].isna().sum())\n",
    "            df[x] = df[x].fillna(0)\n",
    "           #print(\"Mean-\"+str(df[x].mean()))\n",
    "    return df\n",
    "train_transaction_data=nan2mean(train_transaction_data)\n",
    "test_transaction_data=nan2mean(test_transaction_data)\n",
    "\n",
    "\n",
    "category_counts = {}\n",
    "for f in categorical:\n",
    "    train_transaction_data[f] = train_transaction_data[f].replace(\"nan\", \"other\")\n",
    "    train_transaction_data[f] = train_transaction_data[f].replace(np.nan, \"other\")\n",
    "    test_transaction_data[f] = test_transaction_data[f].replace(\"nan\", \"other\")\n",
    "    test_transaction_data[f] = test_transaction_data[f].replace(np.nan, \"other\")\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train_transaction_data[f].values) + list(test_transaction_data[f].values))\n",
    "    train_transaction_data[f] = lbl.transform(list(train_transaction_data[f].values))\n",
    "    test_transaction_data[f] = lbl.transform(list(test_transaction_data[f].values))\n",
    "    category_counts[f] = len(list(lbl.classes_)) + 1\n",
    "# train_transaction_data = train_transaction_data.reset_index()\n",
    "# test_transaction_data = test_transaction_data.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "for column in numerical:\n",
    "    scaler = StandardScaler()\n",
    "    if train_transaction_data[column].max() > 100 and train_transaction_data[column].min() >= 0:\n",
    "        train_transaction_data[column] = np.log1p(train_transaction_data[column])\n",
    "        test_transaction_data[column] = np.log1p(test_transaction_data[column])\n",
    "    scaler.fit(np.concatenate([train_transaction_data[column].values.reshape(-1,1), test_transaction_data[column].values.reshape(-1,1)]))\n",
    "    train_transaction_data[column] = scaler.transform(train_transaction_data[column].values.reshape(-1,1))\n",
    "    test_transaction_data[column] = scaler.transform(test_transaction_data[column].values.reshape(-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_transaction_data.to_pickle('train_transaction_data.pkl')\n",
    "train_identity_data.to_pickle('train_identity_data.pkl')\n",
    "test_transaction_data.to_pickle('test_transaction_data.pkl')\n",
    "test_identity_data.to_pickle('test_identity_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
